# Example configuration for arey app
#
# Looked up from XDG_CONFIG_DIR (~/.config/arey/arey.yml) on Linux or
# ~/.arey on Windows.
#

# Reusable model configurations
# For <=3B models: max out GPU and CPU threads.
max_gpu: &max_gpu
  n_threads: 11
  n_gpu_layers: 99

# For ~7B models
big_gpu: &big_gpu
  n_threads: 11
  n_gpu_layers: 20

# For ~9B models
med_gpu: &med_gpu
  n_threads: 11
  n_gpu_layers: 14

# For ~13B models
small_gpu: &small_gpu
  n_threads: 11
  n_gpu_layers: 14

# Ordered by size and favorites within same size segment
models:
  kimi-k2:
    name: moonshotai/kimi-k2-instruct
    type: openai
    base_url: https://api.groq.com/openai/v1
    api_key: env:GROQ_API_KEY # defined as env variable
  llama-70b:
    name: llama-3.3-70b-versatile
    type: openai
    base_url: https://api.groq.com/openai/v1
    api_key: env:GROQ_API_KEY # defined as env variable
  gemini-flash:
    name: gemini-2.5-flash
    type: openai
    base_url: https://generativelanguage.googleapis.com/v1beta/openai
    api_key: env:GEMINI_API_KEY
  gemini-flash-lite:
    name: gemini-2.5-flash-lite-preview-06-17
    type: openai
    base_url: https://generativelanguage.googleapis.com/v1beta/openai
    api_key: env:GEMINI_API_KEY
  or-kimi-k2:
    name: moonshotai/kimi-k2:free
    type: openai
    base_url: https://openrouter.ai/api/v1
    api_key: env:OPENROUTER_API_KEY
  or-gemma-27b:
    name: deepseek/deepseek-chat-v3-0324:free
    type: openai
    base_url: https://openrouter.ai/api/v1
    api_key: env:OPENROUTER_API_KEY
  llamacpp-server:
    name: gemma2-9b-it-sppo-iter3
    type: openai
    base_url: "http://localhost:8080"
  qwen-4b-instruct:
    <<: *max_gpu
    path: ~/docs/models/qwen/Qwen3-4B-Instruct-2507-UD-Q4_K_XL.gguf
    type: gguf
    n_ctx: 4096
  qwen-4b-think:
    <<: *max_gpu
    path: ~/docs/models/qwen/Qwen3-4B-Thinking-2507-UD-Q4_K_XL.gguf
    type: gguf
    n_ctx: 4096
  jan-4b:
    <<: *max_gpu
    path: ~/docs/models/jan/jan-v1-4B-Q4_K_L.gguf
    type: gguf
    n_ctx: 4096
  lucy-2b:
    <<: *max_gpu
    path: ~/docs/models/jan/jan-lucy-128k-Q6_K_L.gguf
    type: gguf
    n_ctx: 4096
  qwen3-4b:
    <<: *max_gpu
    path: ~/docs/models/qwen/Qwen3-4B-UD-Q4_K_XL.gguf
    type: gguf
  qwen2.5-3b:
    <<: *max_gpu
    path: ~/docs/models/qwen/Qwen2.5-3B-Instruct-Q5_K_L.gguf
    type: gguf
  llama-3.2-3b:
    <<: *max_gpu
    path: ~/docs/models/llama/Llama-3.2-3B-Instruct-Q5_K_L.gguf
    type: gguf
  gemma2-2b-it:
    <<: *max_gpu
    path: ~/docs/models/gemma-2/gemma-2-2b-it-Q6_K_L.gguf
    type: gguf
  phi-3.1-mini-4k-instruct:
    <<: *max_gpu
    path: ~/docs/models/phi-3.1-mini-4k-instruct/Phi-3.1-mini-4k-instruct-Q5_K_M.gguf
    type: gguf
  qwen3-0.6b:
    <<: *max_gpu
    path: ~/docs/models/qwen/Qwen3-0.6B-UD-Q8_K_XL.gguf
    type: gguf
  granite3-8b:
    <<: *big_gpu
    path: ~/docs/models/ibm/granite-3.3-8b-instruct-UD-Q4_K_XL.gguf
    type: gguf
    n_ctx: 4096
  gemma2-9b-atarax:
    <<: *med_gpu
    path: ~/docs/models/gemma-2/Gemma-2-Ataraxy-9B-Q4_K_L.gguf
    type: gguf
  gemma2-9b-it-sppo-iter3:
    <<: *med_gpu
    path: ~/docs/models/gemma-2/Gemma-2-9B-It-SPPO-Iter3-Q4_K_L.gguf
    type: gguf
  gemma2-9b-it:
    <<: *med_gpu
    path: ~/docs/models/gemma-2/gemma-2-9b-it-Q4_K_L.gguf
    type: gguf
  codegeex4-9b:
    <<: *med_gpu
    path: ~/docs/models/codegeex4-all-9b/codegeex4-all-9b-Q4_K_M.gguf
    type: gguf
  llama-3.1-8b-instruct:
    <<: *big_gpu
    path: ~/docs/models/llama-3.1-8b-instruct/Meta-Llama-3.1-8B-Instruct-Q5_K_L.gguf
    type: gguf
  phi-3-medium-4k-instruct:
    <<: *small_gpu
    path: ~/docs/models/phi-3-medium-4k-instruct/Phi-3-medium-4k-instruct-Q4_K_M.gguf
    type: gguf

profiles:
  zero:
    # factual responses and straightforward assistant answers
    temperature: 0
    repeat_penalty: 0.95
    top_k: 20
    top_p: 0.1
  # See https://www.reddit.com/r/LocalLLaMA/comments/1343bgz/what_model_parameters_is_everyone_using/
  precise:
    # factual responses and straightforward assistant answers
    temperature: 0.7
    repeat_penalty: 1.176
    top_k: 40
    top_p: 0.1
  creative:
    # chatting, storywriting, and interesting assistant answers
    temperature: 0.72
    repeat_penalty: 1.1
    top_k: 0
    top_p: 0.73
  sphinx:
    # varied storywriting (on 30B/65B) and unconventional chatting
    temperature: 1.99
    repeat_penalty: 1.15
    top_k: 30
    top_p: 0.18
  qwen-think:
    temperature: 0.6
    repeat_penalty: 1.176
    min_p: 0.0
    top_p: 0.95
    top_k: 20
  qwen:
    temperature: 0.7
    repeat_penalty: 1.176
    min_p: 0.01
    top_p: 0.8
    top_k: 20
  jan:
    temperature: 0.6
    repeat_penalty: 1.176
    top_p: 0.95
    top_k: 20
    min_p: 0
    max_tokens: 2048
  lucy:
    temperature: 0.7
    repeat_penalty: 1.176
    top_p: 0.9
    top_k: 20
    min_p: 0

chat:
  # model: granite3-8b
  # profile: qwen
  # model: qwen-4b-instruct
  # profile: qwen
  # model: jan-4b
  # profile: jan
  model: lucy-2b
  profile: lucy
  # model: kimi-k2
  # profile: precise
  # model: llamacpp-server
  # profile: qwen-think
task:
  model: gemini-flash-lite
  profile: precise

tools:
  search:
    provider: searxng
    base_url: https://search.codito.in

# vim: set foldlevel=99:
